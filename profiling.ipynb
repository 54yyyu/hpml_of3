{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7b9666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "# Import necessary standard libraries\n",
    "import pickle\n",
    "import urllib.request\n",
    "from dataclasses import asdict, dataclass\n",
    "from pathlib import Path\n",
    "from typing import Literal, Optional, Any # Added Any\n",
    "\n",
    "# Import Click for command-line interface (though we'll run it directly)\n",
    "import click\n",
    "\n",
    "# Import PyTorch and PyTorch Lightning\n",
    "import torch\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.strategies import DDPStrategy\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import Boltz specific modules (assuming they are in the PYTHONPATH or same directory)\n",
    "from boltz.data import const\n",
    "from boltz.data.module.inference import BoltzInferenceDataModule\n",
    "from boltz.data.msa.mmseqs2 import run_mmseqs2\n",
    "from boltz.data.parse.a3m import parse_a3m\n",
    "from boltz.data.parse.csv import parse_csv\n",
    "from boltz.data.parse.fasta import parse_fasta\n",
    "from boltz.data.parse.yaml import parse_yaml\n",
    "from boltz.data.types import MSA, Manifest, Record\n",
    "from boltz.data.write.writer import BoltzWriter\n",
    "from boltz.model.model import Boltz1\n",
    "\n",
    "# Import PyTorch Profiler\n",
    "import torch.profiler\n",
    "import os # For creating profiler output directory\n",
    "\n",
    "# Define constants for URLs\n",
    "CCD_URL = \"https://huggingface.co/boltz-community/boltz-1/resolve/main/ccd.pkl\"\n",
    "MODEL_URL = (\n",
    "    \"https://huggingface.co/boltz-community/boltz-1/resolve/main/boltz1_conf.ckpt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70490f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dataclass Definitions\n",
    "@dataclass\n",
    "class BoltzProcessedInput:\n",
    "    \"\"\"Processed input data.\"\"\"\n",
    "\n",
    "    manifest: Manifest\n",
    "    targets_dir: Path\n",
    "    msa_dir: Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BoltzDiffusionParams:\n",
    "    \"\"\"Diffusion process parameters.\"\"\"\n",
    "\n",
    "    gamma_0: float = 0.605\n",
    "    gamma_min: float = 1.107\n",
    "    noise_scale: float = 0.901\n",
    "    rho: float = 8\n",
    "    step_scale: float = 1.638\n",
    "    sigma_min: float = 0.0004\n",
    "    sigma_max: float = 160.0\n",
    "    sigma_data: float = 16.0\n",
    "    P_mean: float = -1.2\n",
    "    P_std: float = 1.5\n",
    "    coordinate_augmentation: bool = True\n",
    "    alignment_reverse_diff: bool = True\n",
    "    synchronize_sigmas: bool = True\n",
    "    use_inference_model_cache: bool = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0268472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Download Function\n",
    "@rank_zero_only\n",
    "def download(cache: Path) -> None:\n",
    "    \"\"\"Download all the required data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cache : Path\n",
    "        The cache directory.\n",
    "\n",
    "    \"\"\"\n",
    "    # Download CCD\n",
    "    ccd = cache / \"ccd.pkl\"\n",
    "    if not ccd.exists():\n",
    "        click.echo(\n",
    "            f\"Downloading the CCD dictionary to {ccd}. You may \"\n",
    "            \"change the cache directory with the --cache flag.\"\n",
    "        )\n",
    "        urllib.request.urlretrieve(CCD_URL, str(ccd))\n",
    "\n",
    "    # Download model\n",
    "    model = cache / \"boltz1_conf.ckpt\"\n",
    "    if not model.exists():\n",
    "        click.echo(\n",
    "            f\"Downloading the model weights to {model}. You may \"\n",
    "            \"change the cache directory with the --cache flag.\"\n",
    "        )\n",
    "        urllib.request.urlretrieve(MODEL_URL, str(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe8ee899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Input Checking Function\n",
    "def check_inputs(\n",
    "    data: Path,\n",
    "    outdir: Path,\n",
    "    override: bool = False,\n",
    ") -> list[Path]:\n",
    "    \"\"\"Check the input data and output directory.\n",
    "\n",
    "    If the input data is a directory, it will be expanded\n",
    "    to all files in this directory. Then, we check if there\n",
    "    are any existing predictions and remove them from the\n",
    "    list of input data, unless the override flag is set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Path\n",
    "        The input data.\n",
    "    outdir : Path\n",
    "        The output directory.\n",
    "    override: bool\n",
    "        Whether to override existing predictions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[Path]\n",
    "        The list of input data.\n",
    "\n",
    "    \"\"\"\n",
    "    click.echo(\"Checking input data.\")\n",
    "\n",
    "    if data.is_dir():\n",
    "        data_files: list[Path] = list(data.glob(\"*\"))\n",
    "        filtered_data = []\n",
    "        for d_file in data_files:\n",
    "            if d_file.suffix in (\".fa\", \".fas\", \".fasta\", \".yml\", \".yaml\"):\n",
    "                filtered_data.append(d_file)\n",
    "            elif d_file.is_dir():\n",
    "                msg = f\"Found directory {d_file} instead of .fasta or .yaml.\"\n",
    "                raise RuntimeError(msg)\n",
    "            else:\n",
    "                msg = (\n",
    "                    f\"Unable to parse filetype {d_file.suffix}, \"\n",
    "                    \"please provide a .fasta or .yaml file.\"\n",
    "                )\n",
    "                raise RuntimeError(msg)\n",
    "        data_list = filtered_data\n",
    "    else:\n",
    "        data_list = [data]\n",
    "\n",
    "    existing = (outdir / \"predictions\").rglob(\"*\")\n",
    "    existing_stems = {e.stem for e in existing if e.is_dir()} # Use stem for comparison\n",
    "\n",
    "    if existing_stems and not override:\n",
    "        original_count = len(data_list)\n",
    "        data_list = [d for d in data_list if d.stem not in existing_stems]\n",
    "        num_skipped = original_count - len(data_list)\n",
    "        if num_skipped > 0:\n",
    "            msg = (\n",
    "                f\"Found {num_skipped} existing predictions, \"\n",
    "                f\"skipping and running only the missing ones ({len(data_list)}), \"\n",
    "                \"if any. If you wish to override these existing \"\n",
    "                \"predictions, please set the --override flag.\"\n",
    "            )\n",
    "            click.echo(msg)\n",
    "    elif existing_stems and override:\n",
    "        msg = f\"Found {len(existing_stems)} existing predictions, will override.\"\n",
    "        click.echo(msg)\n",
    "\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "274dfbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: MSA Computation Function\n",
    "def compute_msa(\n",
    "    data: dict[str, str],\n",
    "    target_id: str,\n",
    "    msa_dir: Path,\n",
    "    msa_server_url: str,\n",
    "    msa_pairing_strategy: str,\n",
    ") -> None:\n",
    "    \"\"\"Compute the MSA for the input data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict[str, str]\n",
    "        The input protein sequences.\n",
    "    target_id : str\n",
    "        The target id.\n",
    "    msa_dir : Path\n",
    "        The msa directory.\n",
    "    msa_server_url : str\n",
    "        The MSA server URL.\n",
    "    msa_pairing_strategy : str\n",
    "        The MSA pairing strategy.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(data) > 1:\n",
    "        paired_msas = run_mmseqs2(\n",
    "            list(data.values()),\n",
    "            msa_dir / f\"{target_id}_paired_tmp\",\n",
    "            use_env=True,\n",
    "            use_pairing=True,\n",
    "            host_url=msa_server_url,\n",
    "            pairing_strategy=msa_pairing_strategy,\n",
    "        )\n",
    "    else:\n",
    "        paired_msas = [\"\"] * len(data)\n",
    "\n",
    "    unpaired_msa = run_mmseqs2(\n",
    "        list(data.values()),\n",
    "        msa_dir / f\"{target_id}_unpaired_tmp\",\n",
    "        use_env=True,\n",
    "        use_pairing=False,\n",
    "        host_url=msa_server_url,\n",
    "        pairing_strategy=msa_pairing_strategy,\n",
    "    )\n",
    "\n",
    "    for idx, name in enumerate(data):\n",
    "        paired = paired_msas[idx].strip().splitlines()\n",
    "        paired = paired[1::2]\n",
    "        paired = paired[: const.max_paired_seqs]\n",
    "\n",
    "        keys = [i for i, s in enumerate(paired) if s != \"-\" * len(s)]\n",
    "        paired = [s for s in paired if s != \"-\" * len(s)]\n",
    "\n",
    "        unpaired = unpaired_msa[idx].strip().splitlines()\n",
    "        unpaired = unpaired[1::2]\n",
    "        unpaired = unpaired[: (const.max_msa_seqs - len(paired))]\n",
    "        if paired:\n",
    "            unpaired = unpaired[1:]\n",
    "\n",
    "        seqs = paired + unpaired\n",
    "        keys = keys + [-1] * len(unpaired)\n",
    "\n",
    "        csv_str = [\"key,sequence\"] + [f\"{key},{seq}\" for key, seq in zip(keys, seqs)]\n",
    "\n",
    "        msa_path = msa_dir / f\"{name}.csv\"\n",
    "        with msa_path.open(\"w\") as f:\n",
    "            f.write(\"\\n\".join(csv_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28a0d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Input Processing Function\n",
    "@rank_zero_only\n",
    "def process_inputs(\n",
    "    data: list[Path],\n",
    "    out_dir: Path,\n",
    "    ccd_path: Path,\n",
    "    msa_server_url: str,\n",
    "    msa_pairing_strategy: str,\n",
    "    max_msa_seqs: int = 4096,\n",
    "    use_msa_server: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Process the input data and output directory.\n",
    "    (Function body remains the same as in your original script)\n",
    "    \"\"\"\n",
    "    click.echo(\"Processing input data.\")\n",
    "    existing_records = None\n",
    "\n",
    "    manifest_path = out_dir / \"processed\" / \"manifest.json\"\n",
    "    if manifest_path.exists():\n",
    "        click.echo(f\"Found a manifest file at output directory: {out_dir}\")\n",
    "        manifest_data: Manifest = Manifest.load(manifest_path)\n",
    "        input_ids = [d.stem for d in data]\n",
    "        \n",
    "        # Ensure manifest_data.records is not None before proceeding\n",
    "        if manifest_data.records is None:\n",
    "            manifest_data.records = []\n",
    "\n",
    "        processed_records_tuples = [\n",
    "            (record, record.id)\n",
    "            for record in manifest_data.records\n",
    "            if record.id in input_ids\n",
    "        ]\n",
    "        \n",
    "        if processed_records_tuples:\n",
    "            existing_records, processed_ids = zip(*processed_records_tuples)\n",
    "            existing_records = list(existing_records)\n",
    "        else:\n",
    "            existing_records = []\n",
    "            processed_ids = []\n",
    "\n",
    "\n",
    "        missing = len(input_ids) - len(processed_ids)\n",
    "        if not missing:\n",
    "            click.echo(\"All examples in data are processed. Updating the manifest.\")\n",
    "            if existing_records is not None:\n",
    "                 updated_manifest = Manifest(existing_records)\n",
    "                 updated_manifest.dump(out_dir / \"processed\" / \"manifest.json\")\n",
    "            return\n",
    "\n",
    "        click.echo(f\"{missing} missing ids. Preprocessing these ids.\")\n",
    "        missing_ids = list(set(input_ids).difference(set(processed_ids)))\n",
    "        data = [d for d in data if d.stem in missing_ids]\n",
    "        assert len(data) == len(missing_ids)\n",
    "\n",
    "    msa_dir = out_dir / \"msa\"\n",
    "    structure_dir = out_dir / \"processed\" / \"structures\"\n",
    "    processed_msa_dir = out_dir / \"processed\" / \"msa\"\n",
    "    predictions_dir = out_dir / \"predictions\"\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    msa_dir.mkdir(parents=True, exist_ok=True)\n",
    "    structure_dir.mkdir(parents=True, exist_ok=True)\n",
    "    processed_msa_dir.mkdir(parents=True, exist_ok=True)\n",
    "    predictions_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with ccd_path.open(\"rb\") as file:\n",
    "        ccd = pickle.load(file)\n",
    "\n",
    "    if existing_records:\n",
    "        click.echo(f\"Found {len(existing_records)} records. Adding them to records.\")\n",
    "\n",
    "    records: list[Record] = list(existing_records) if existing_records is not None else []\n",
    "    for path in tqdm(data):\n",
    "        try:\n",
    "            if path.suffix in (\".fa\", \".fas\", \".fasta\"):\n",
    "                target = parse_fasta(path, ccd)\n",
    "            elif path.suffix in (\".yml\", \".yaml\"):\n",
    "                target = parse_yaml(path, ccd)\n",
    "            else:\n",
    "                msg = f\"Unsupported file type: {path.suffix}\"\n",
    "                raise RuntimeError(msg)\n",
    "\n",
    "            target_id = target.record.id\n",
    "            to_generate = {}\n",
    "            prot_id = const.chain_type_ids[\"PROTEIN\"]\n",
    "            for chain in target.record.chains:\n",
    "                if (chain.mol_type == prot_id) and (chain.msa_id == 0):\n",
    "                    entity_id = chain.entity_id\n",
    "                    msa_id_name = f\"{target_id}_{entity_id}\"\n",
    "                    to_generate[msa_id_name] = target.sequences[entity_id]\n",
    "                    chain.msa_id = msa_dir / f\"{msa_id_name}.csv\"\n",
    "                elif chain.msa_id == 0:\n",
    "                    chain.msa_id = -1 # type: ignore\n",
    "\n",
    "            if to_generate and not use_msa_server:\n",
    "                msg = \"Missing MSA's in input and --use_msa_server flag not set.\"\n",
    "                raise RuntimeError(msg)\n",
    "\n",
    "            if to_generate:\n",
    "                click.echo(f\"Generating MSA for {path} with {len(to_generate)} protein entities.\")\n",
    "                compute_msa(\n",
    "                    data=to_generate,\n",
    "                    target_id=target_id,\n",
    "                    msa_dir=msa_dir,\n",
    "                    msa_server_url=msa_server_url,\n",
    "                    msa_pairing_strategy=msa_pairing_strategy,\n",
    "                )\n",
    "\n",
    "            msas_paths = sorted({c.msa_id for c in target.record.chains if c.msa_id != -1 and c.msa_id != 0}) # Ensure c.msa_id is Path-like\n",
    "            msa_id_map = {}\n",
    "            for msa_idx, msa_p_obj in enumerate(msas_paths):\n",
    "                msa_path = Path(msa_p_obj) # Ensure it's a Path object\n",
    "                if not msa_path.exists():\n",
    "                    msg = f\"MSA file {msa_path} not found.\"\n",
    "                    raise FileNotFoundError(msg)\n",
    "\n",
    "                processed = processed_msa_dir / f\"{target_id}_{msa_idx}.npz\"\n",
    "                msa_id_map[msa_p_obj] = f\"{target_id}_{msa_idx}\" # type: ignore\n",
    "                if not processed.exists():\n",
    "                    if msa_path.suffix == \".a3m\":\n",
    "                        msa: MSA = parse_a3m(msa_path, taxonomy=None, max_seqs=max_msa_seqs)\n",
    "                    elif msa_path.suffix == \".csv\":\n",
    "                        msa: MSA = parse_csv(msa_path, max_seqs=max_msa_seqs)\n",
    "                    else:\n",
    "                        msg = f\"MSA file {msa_path} not supported, only a3m or csv.\"\n",
    "                        raise RuntimeError(msg)\n",
    "                    msa.dump(processed)\n",
    "\n",
    "            for c in target.record.chains:\n",
    "                if (c.msa_id != -1 and c.msa_id != 0) and (c.msa_id in msa_id_map):\n",
    "                    c.msa_id = msa_id_map[c.msa_id] # type: ignore\n",
    "\n",
    "            records.append(target.record)\n",
    "            struct_path = structure_dir / f\"{target.record.id}.npz\"\n",
    "            target.structure.dump(struct_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            if len(data) > 1:\n",
    "                print(f\"Failed to process {path}. Skipping. Error: {e}.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    manifest = Manifest(records)\n",
    "    manifest.dump(out_dir / \"processed\" / \"manifest.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05151d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Prediction and Profiling Function\n",
    "def predict_and_profile( # Renamed for clarity\n",
    "    data_path_str: str, # Changed 'data' to 'data_path_str' to avoid conflict\n",
    "    out_dir_str: str,   # Changed 'out_dir' to 'out_dir_str'\n",
    "    cache_str: str = \"~/.boltz\",\n",
    "    checkpoint: Optional[str] = None,\n",
    "    # devices: int = 1, # Simplified for single device profiling for now\n",
    "    accelerator: str = \"gpu\", # \"cpu\" or \"gpu\"\n",
    "    recycling_steps: int = 3,\n",
    "    sampling_steps: int = 200, # Reduced for faster profiling example\n",
    "    diffusion_samples: int = 1, # Reduced for faster profiling\n",
    "    step_scale: float = 1.638,\n",
    "    # write_full_pae: bool = False, # Commented out unused params for this example\n",
    "    # write_full_pde: bool = False,\n",
    "    # output_format: Literal[\"pdb\", \"mmcif\"] = \"mmcif\",\n",
    "    num_workers: int = 0, # Often 0 for notebooks to avoid issues with multiprocessing\n",
    "    override: bool = False,\n",
    "    seed_val: Optional[int] = None, # Renamed from 'seed'\n",
    "    use_msa_server: bool = False,\n",
    "    msa_server_url: str = \"https://api.colabfold.com\",\n",
    "    msa_pairing_strategy: str = \"greedy\",\n",
    "    profile_output_dir: str = \"./profiler_output\" # Directory for profiler traces\n",
    ") -> None:\n",
    "    \"\"\"Run a single prediction step with Boltz-1 and profile memory usage.\"\"\"\n",
    "    \n",
    "    # Determine device for PyTorch\n",
    "    if accelerator == \"cpu\":\n",
    "        torch_device = torch.device(\"cpu\")\n",
    "        profiler_activities = [torch.profiler.ProfilerActivity.CPU]\n",
    "        click.echo(\"Running on CPU. Profiling CPU activity.\")\n",
    "    elif accelerator == \"gpu\" and torch.cuda.is_available():\n",
    "        torch_device = torch.device(\"cuda:0\") # Assuming first GPU for simplicity\n",
    "        profiler_activities = [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA]\n",
    "        click.echo(f\"Running on GPU: {torch_device}. Profiling CPU and CUDA activity.\")\n",
    "    else:\n",
    "        if accelerator == \"gpu\":\n",
    "            click.echo(\"CUDA not available, falling back to CPU.\")\n",
    "        torch_device = torch.device(\"cpu\")\n",
    "        profiler_activities = [torch.profiler.ProfilerActivity.CPU]\n",
    "        accelerator = \"cpu\" # Ensure consistency\n",
    "        click.echo(\"Running on CPU. Profiling CPU activity.\")\n",
    "\n",
    "    # Set no grad\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    # Ignore matmul precision warning\n",
    "    torch.set_float32_matmul_precision(\"highest\")\n",
    "\n",
    "    # Set seed if desired\n",
    "    if seed_val is not None:\n",
    "        seed_everything(int(seed_val))\n",
    "\n",
    "    # Set cache path\n",
    "    cache = Path(cache_str).expanduser()\n",
    "    cache.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create output directories\n",
    "    data_path = Path(data_path_str).expanduser()\n",
    "    out_dir = Path(out_dir_str).expanduser()\n",
    "    # Use a more specific output directory name to avoid clashes if data_path_str is a dir\n",
    "    name_stem = data_path.stem if data_path.is_file() else data_path.name\n",
    "    out_dir = out_dir / f\"boltz_results_{name_stem}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create profiler output directory\n",
    "    Path(profile_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Download necessary data and model\n",
    "    download(cache)\n",
    "\n",
    "    # Validate inputs\n",
    "    # For profiling a single forward pass, we expect a single input file\n",
    "    # The check_inputs function might filter it out if results exist and override is False.\n",
    "    # We'll ensure we get at least one item or handle it.\n",
    "    input_files = check_inputs(data_path, out_dir, override)\n",
    "    if not input_files:\n",
    "        click.echo(\"No input files to process after checking existing outputs. Exiting.\")\n",
    "        # If you expect `check_inputs` to return the original path even if processed (for re-profiling)\n",
    "        # you might need to adjust `check_inputs` or handle it here.\n",
    "        # For now, we'll assume if `input_files` is empty, we can't proceed.\n",
    "        if data_path.is_file() and not override:\n",
    "             click.echo(f\"The file {data_path} might have been processed. Use --override or clean outputs.\")\n",
    "        return\n",
    "    \n",
    "    # We will profile prediction for the first file in the list.\n",
    "    # If `data_path_str` was a directory, `process_inputs` will handle all,\n",
    "    # but for focused profiling, we'll get a batch from the datamodule based on this processing.\n",
    "    # The current `process_inputs` is @rank_zero_only and processes all `input_files`.\n",
    "\n",
    "    click.echo(f\"Running predictions for {len(input_files)} structure(s)\")\n",
    "    if len(input_files) > 1:\n",
    "        click.echo(\"Note: Profiling will focus on one batch from the first processed input.\")\n",
    "\n",
    "    # Process inputs (generates manifest.json, etc.)\n",
    "    ccd_path = cache / \"ccd.pkl\"\n",
    "    process_inputs( # This will process all files in input_files\n",
    "        data=input_files, # Pass the potentially filtered list\n",
    "        out_dir=out_dir,\n",
    "        ccd_path=ccd_path,\n",
    "        use_msa_server=use_msa_server,\n",
    "        msa_server_url=msa_server_url,\n",
    "        msa_pairing_strategy=msa_pairing_strategy,\n",
    "    )\n",
    "\n",
    "    # Load processed data\n",
    "    processed_dir = out_dir / \"processed\"\n",
    "    manifest_file = processed_dir / \"manifest.json\"\n",
    "    if not manifest_file.exists():\n",
    "        click.echo(f\"Manifest file not found at {manifest_file} after processing. Cannot proceed with profiling.\")\n",
    "        return\n",
    "        \n",
    "    processed = BoltzProcessedInput(\n",
    "        manifest=Manifest.load(manifest_file),\n",
    "        targets_dir=processed_dir / \"structures\",\n",
    "        msa_dir=processed_dir / \"msa\",\n",
    "    )\n",
    "    \n",
    "    if not processed.manifest.records:\n",
    "        click.echo(\"No records found in the manifest. Cannot create dataloader for profiling.\")\n",
    "        return\n",
    "\n",
    "    # Create data module\n",
    "    data_module = BoltzInferenceDataModule(\n",
    "        manifest=processed.manifest,\n",
    "        target_dir=processed.targets_dir,\n",
    "        msa_dir=processed.msa_dir,\n",
    "        num_workers=num_workers,\n",
    "        # For profiling, usually batch_size = 1 is good for isolating single sample behavior\n",
    "        # However, BoltzInferenceDataModule might have its own batch size logic.\n",
    "        # We will take the default from the module.\n",
    "    )\n",
    "    data_module.setup(stage='predict') # Call setup to prepare dataloaders\n",
    "\n",
    "    # Load model\n",
    "    if checkpoint is None:\n",
    "        checkpoint_path = cache / \"boltz1_conf.ckpt\"\n",
    "    else:\n",
    "        checkpoint_path = Path(checkpoint)\n",
    "\n",
    "    predict_args = {\n",
    "        \"recycling_steps\": recycling_steps,\n",
    "        \"sampling_steps\": sampling_steps,\n",
    "        \"diffusion_samples\": diffusion_samples,\n",
    "        \"write_confidence_summary\": False, # Keep false for focused profiling\n",
    "        \"write_full_pae\": False,\n",
    "        \"write_full_pde\": False,\n",
    "    }\n",
    "    diffusion_params = BoltzDiffusionParams()\n",
    "    diffusion_params.step_scale = step_scale\n",
    "    \n",
    "    model_module: Boltz1 = Boltz1.load_from_checkpoint(\n",
    "        checkpoint_path,\n",
    "        strict=True,\n",
    "        predict_args=predict_args,\n",
    "        map_location=\"cpu\", # Load to CPU first, then move to device\n",
    "        diffusion_process_args=asdict(diffusion_params),\n",
    "        ema=False,\n",
    "        steering_args={\"fk_steering\": False,\n",
    "                       \"guidance_update\": False,\n",
    "                       \"num_particles\": 3,\n",
    "                       \"fk_lambda\": 4.0},\n",
    "    )\n",
    "    model_module.eval()\n",
    "    model_module.to(torch_device) # Move model to the target device\n",
    "\n",
    "    click.echo(f\"Model loaded and moved to {torch_device}.\")\n",
    "\n",
    "    # Get a single batch from the dataloader\n",
    "    try:\n",
    "        dataloader = data_module.predict_dataloader()\n",
    "        if not dataloader:\n",
    "            click.echo(\"Predict dataloader is None or empty. Cannot get a batch for profiling.\")\n",
    "            return\n",
    "        batch = next(iter(dataloader))\n",
    "    except Exception as e:\n",
    "        click.echo(f\"Error getting a batch from the dataloader: {e}\")\n",
    "        click.echo(\"Ensure your input data can be processed into at least one batch.\")\n",
    "        return\n",
    "\n",
    "    # Move batch to the target device\n",
    "    # The batch structure depends on BoltzInferenceDataModule's collate_fn\n",
    "    # Assuming batch is a dict, list, or tensor that needs to be moved\n",
    "    if isinstance(batch, dict):\n",
    "        batch = {k: v.to(torch_device) if hasattr(v, 'to') else v for k, v in batch.items()}\n",
    "    elif isinstance(batch, (list, tuple)):\n",
    "        batch = [b.to(torch_device) if hasattr(b, 'to') else b for b in batch]\n",
    "    elif hasattr(batch, 'to'):\n",
    "        batch = batch.to(torch_device)\n",
    "    \n",
    "    click.echo(\"Single batch obtained and moved to device.\")\n",
    "\n",
    "    # Define the profiler trace handler\n",
    "    # trace_handler = torch.profiler.tensorboard_trace_handler(profile_output_dir)\n",
    "    \n",
    "    click.echo(f\"Starting profiling for a single predict_step. Trace will be saved to {profile_output_dir}\")\n",
    "\n",
    "    # # Profile the model's predict_step (which includes the forward pass)\n",
    "    # with torch.profiler.profile(\n",
    "    #     activities=profiler_activities,\n",
    "    #     record_shapes=True,      # Records tensor shapes\n",
    "    #     profile_memory=True,     # Enables memory profiling\n",
    "    #     with_stack=True,         # Records callstacks\n",
    "    #     on_trace_ready=trace_handler # Handles the trace when profiling is done\n",
    "    # ) as prof:\n",
    "    #     with torch.no_grad(): # Ensure no gradients are computed during this specific step\n",
    "    #         # The predict_step in LightningModules usually takes (self, batch, batch_idx, dataloader_idx=0)\n",
    "    #         # We simulate a call for the first batch (batch_idx=0)\n",
    "    torch.cuda.memory._record_memory_history()\n",
    "    _ = model_module.predict_step(batch, batch_idx=0) \n",
    "    torch.cuda.memory._dump_snapshot(f\"{profile_output_dir}/result.pickle\") # Dump memory snapshot\n",
    "    # We don't need the output for profiling, just the execution.\n",
    "\n",
    "    click.echo(\"Profiling finished.\")\n",
    "    click.echo(f\"Profiler trace saved in '{profile_output_dir}'.\")\n",
    "    click.echo(\"You can view it using TensorBoard: tensorboard --logdir=\" + profile_output_dir)\n",
    "\n",
    "    # # Print a summary of memory usage to the console\n",
    "    # click.echo(\"\\n--- CPU Memory Usage Summary (Self) ---\")\n",
    "    # click.echo(prof.key_averages(group_by_input_shape=True).table(sort_by=\"self_cpu_memory_usage\", row_limit=15))\n",
    "    \n",
    "    # if torch.profiler.ProfilerActivity.CUDA in profiler_activities:\n",
    "    #     click.echo(\"\\n--- CUDA Memory Usage Summary (Self) ---\")\n",
    "    #     click.echo(prof.key_averages(group_by_input_shape=True).table(sort_by=\"self_cuda_memory_usage\", row_limit=15))\n",
    "        \n",
    "    # click.echo(\"\\n--- Operator Table (CPU time) ---\")\n",
    "    # click.echo(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a9bdb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Main execution block (for script or notebook cell)\n",
    "# if __name__ == \"__main__\":\n",
    "# Create dummy input file for the example if it doesn't exist\n",
    "# In a real scenario, replace this with your actual input data path\n",
    "example_input_dir = Path(\"./inputs_prediction_profiling\")\n",
    "example_input_dir.mkdir(parents=True, exist_ok=True)\n",
    "example_fasta_file = example_input_dir / \"example.fasta\"\n",
    "\n",
    "if not example_fasta_file.exists():\n",
    "    with open(example_fasta_file, \"w\") as f:\n",
    "        f.write(\">protein1\\n\")\n",
    "        f.write(\"MILKADLINSLKNVFKSLENSESGSESSENSKENESGHSGSKRKRKPKSSSLLEARMELLLEKREKTKK\\n\") # Example sequence\n",
    "    print(f\"Created dummy input file: {example_fasta_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9275a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters for the prediction and profiling ---\n",
    "# Adjust these paths and parameters as needed for your setup\n",
    "\n",
    "# Path to your input data (single .fasta, .yaml, or a directory)\n",
    "# For this example, using the dummy file created above.\n",
    "# IMPORTANT: For MSA generation to work without a server (use_msa_server=False),\n",
    "# you would typically provide precomputed MSAs or point to a local MMseqs2 setup.\n",
    "# The default `compute_msa` will try to call `run_mmseqs2`.\n",
    "# If you don't have MMseqs2 setup and `use_msa_server=False`, `process_inputs` might fail\n",
    "# during MSA generation. For simplicity, ensure your input YAML specifies MSA files,\n",
    "# or use `use_msa_server=True` if ColabFold API is accessible and suitable.\n",
    "#\n",
    "# For this example to run somewhat out-of-the-box, let's try with use_msa_server=True.\n",
    "# If you have local MSAs, adjust your input or set use_msa_server=False.\n",
    "\n",
    "input_data_path = str(example_fasta_file) # Can be a dir too\n",
    "\n",
    "output_directory = \"./outputs_prediction_profiling\"\n",
    "cache_directory = \"./rna-prediction-boltz-profiling/\" # Separate cache for this run\n",
    "\n",
    "# Profiler specific\n",
    "profiler_trace_output_dir = \"./profiler_trace_boltz\"\n",
    "\n",
    "# Model and run parameters (can be tuned)\n",
    "# For profiling, it's often good to reduce steps for faster feedback initially\n",
    "num_sampling_steps = 50  # Reduced from 200 for quicker profiling run\n",
    "num_diffusion_samples = 1 # Usually 1 for structure prediction profiling\n",
    "\n",
    "# Set override to True if you want to re-process inputs and re-predict\n",
    "# Set to False to use existing processed data/predictions if available.\n",
    "# For profiling, True is often useful to ensure a fresh run.\n",
    "override_existing = True \n",
    "\n",
    "# Set a seed for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "# Choose accelerator: \"cpu\" or \"gpu\"\n",
    "# If \"gpu\" is chosen, it will try to use CUDA.\n",
    "# Ensure PyTorch with CUDA support is installed if you choose \"gpu\".\n",
    "accelerator_type = \"gpu\" # Change to \"gpu\" if you have a compatible GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ae97c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Prediction and Profiling ---\n",
      "Running on GPU: cuda:0. Profiling CPU and CUDA activity.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking input data.\n",
      "Running predictions for 1 structure(s)\n",
      "Processing input data.\n",
      "Found a manifest file at output directory: outputs_prediction_profiling/boltz_results_example\n",
      "All examples in data are processed. Updating the manifest.\n",
      "Model loaded and moved to cuda:0.\n",
      "Single batch obtained and moved to device.\n",
      "Starting profiling for a single predict_step. Trace will be saved to ./profiler_trace_boltz\n",
      "Profiling finished.\n",
      "Profiler trace saved in './profiler_trace_boltz'.\n",
      "You can view it using TensorBoard: tensorboard --logdir=./profiler_trace_boltz\n",
      "--- Prediction and Profiling Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "click.echo(\"--- Starting Prediction and Profiling ---\")\n",
    "\n",
    "# Ensure the dummy const module exists if your boltz.data.const is structured that way\n",
    "# This is a placeholder for what might be in your actual `boltz.data.const`\n",
    "if not hasattr(const, 'max_paired_seqs'):\n",
    "    const.max_paired_seqs = 512 # Example value\n",
    "if not hasattr(const, 'max_msa_seqs'):\n",
    "    const.max_msa_seqs = 1024 # Example value\n",
    "if not hasattr(const, 'chain_type_ids'):\n",
    "    const.chain_type_ids = {\"PROTEIN\": 0, \"RNA\": 1, \"DNA\": 2, \"LIGAND\":3} # Example values\n",
    "\n",
    "predict_and_profile(\n",
    "    data_path_str=input_data_path,\n",
    "    out_dir_str=output_directory,\n",
    "    cache_str=cache_directory,\n",
    "    # checkpoint=None, # Use default from cache\n",
    "    accelerator=accelerator_type,\n",
    "    recycling_steps=1, # Further reduce for speed\n",
    "    sampling_steps=num_sampling_steps,\n",
    "    diffusion_samples=num_diffusion_samples,\n",
    "    # step_scale=1.638, # Default\n",
    "    num_workers=0, # Safer for notebooks/profiling\n",
    "    override=override_existing,\n",
    "    seed_val=random_seed,\n",
    "    use_msa_server=True, # TRY USING SERVER for easier setup, requires internet\n",
    "                        # Set to False if you have local MMseqs2 and want to use it,\n",
    "                        # or if your input YAMLs provide pre-computed MSAs.\n",
    "    msa_server_url=\"https://api.colabfold.com\", # Default\n",
    "    msa_pairing_strategy=\"greedy\", # Default\n",
    "    profile_output_dir=profiler_trace_output_dir\n",
    ")\n",
    "click.echo(\"--- Prediction and Profiling Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0caefa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda.memory # For CUDA memory utilities\n",
    "from pathlib import Path\n",
    "import click # Or use regular function arguments if not using Click for this part\n",
    "from pytorch_lightning import seed_everything\n",
    "# ... other Boltz imports ...\n",
    "from boltz.model.model import Boltz1\n",
    "from boltz.data.module.inference import BoltzInferenceDataModule\n",
    "from boltz.data.types import Manifest # Ensure Manifest is imported for BoltzProcessedInput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5be3730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# New function for profiling a training step using CUDA memory history\n",
    "def profile_training_step_cuda_memory(\n",
    "    data_path_str: str,\n",
    "    out_dir_str: str,\n",
    "    cache_str: str = \"~/.boltz\",\n",
    "    checkpoint: Optional[str] = None,\n",
    "    accelerator: str = \"gpu\", # Must be \"gpu\" for this function\n",
    "    num_workers: int = 0,\n",
    "    override: bool = False,\n",
    "    seed_val: Optional[int] = None,\n",
    "    use_msa_server: bool = False,\n",
    "    msa_server_url: str = \"https://api.colabfold.com\",\n",
    "    msa_pairing_strategy: str = \"greedy\",\n",
    "    profile_output_dir: str = \"./cuda_memory_profile_output\", # Directory for the snapshot pickle\n",
    "    # Boltz1 model specific params (can be defaults or passed if needed for model loading)\n",
    "    recycling_steps: int = 1, # Reduced for speed, adjust if necessary\n",
    "    sampling_steps: int = 50,\n",
    "    diffusion_samples: int = 1,\n",
    "    step_scale: float = 1.638,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Simulates a training step (forward + backward) and profiles CUDA memory\n",
    "    using torch.cuda.memory._record_memory_history and _dump_snapshot.\n",
    "    Reports peak CUDA memory usage.\n",
    "    \"\"\"\n",
    "    click.echo(\"--- Starting Training Step CUDA Memory Profiling ---\")\n",
    "\n",
    "    # This profiler is CUDA-specific\n",
    "    if accelerator != \"gpu\":\n",
    "        click.secho(\"Error: CUDA memory profiling requires accelerator='gpu'.\", fg=\"red\")\n",
    "        return\n",
    "    if not torch.cuda.is_available():\n",
    "        click.secho(\"Error: CUDA is not available. Cannot perform CUDA memory profiling.\", fg=\"red\")\n",
    "        return\n",
    "    \n",
    "    torch_device = torch.device(\"cuda:0\") # Assuming first GPU, or allow selection\n",
    "    click.echo(f\"Using device: {torch_device}\")\n",
    "\n",
    "    # Enable gradients for training\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    if seed_val is not None:\n",
    "        seed_everything(int(seed_val))\n",
    "        click.echo(f\"Random seed set to: {seed_val}\")\n",
    "\n",
    "    cache = Path(cache_str).expanduser()\n",
    "    cache.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    data_path = Path(data_path_str).expanduser()\n",
    "    # Use a more specific output directory name\n",
    "    name_stem = data_path.stem if data_path.is_file() else data_path.name\n",
    "    # Main output directory for general results (if any beyond profile)\n",
    "    out_dir = Path(out_dir_str).expanduser() / f\"boltz_training_profile_results_{name_stem}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Specific directory for the memory snapshot\n",
    "    profile_out_path = Path(profile_output_dir).resolve()\n",
    "    profile_out_path.mkdir(parents=True, exist_ok=True)\n",
    "    click.echo(f\"Memory snapshot will be saved to: {profile_out_path}\")\n",
    "\n",
    "    download(cache)\n",
    "    input_files = check_inputs(data_path, out_dir, override)\n",
    "    if not input_files:\n",
    "        click.echo(\"No input files to process. Exiting.\")\n",
    "        return\n",
    "\n",
    "    click.echo(f\"Processing inputs for {len(input_files)} structure(s)...\")\n",
    "    ccd_path = cache / \"ccd.pkl\"\n",
    "    process_inputs(\n",
    "        data=input_files,\n",
    "        out_dir=out_dir,\n",
    "        ccd_path=ccd_path,\n",
    "        use_msa_server=use_msa_server,\n",
    "        msa_server_url=msa_server_url,\n",
    "        msa_pairing_strategy=msa_pairing_strategy,\n",
    "    )\n",
    "\n",
    "    processed_dir = out_dir / \"processed\"\n",
    "    manifest_file = processed_dir / \"manifest.json\"\n",
    "    if not manifest_file.exists():\n",
    "        click.secho(f\"Manifest file not found at {manifest_file}. Cannot proceed.\", fg=\"red\")\n",
    "        return\n",
    "        \n",
    "    processed_input_data = BoltzProcessedInput(\n",
    "        manifest=Manifest.load(manifest_file),\n",
    "        targets_dir=processed_dir / \"structures\",\n",
    "        msa_dir=processed_dir / \"msa\",\n",
    "    )\n",
    "    \n",
    "    if not processed_input_data.manifest.records:\n",
    "        click.secho(\"No records found in the manifest. Cannot create dataloader.\", fg=\"red\")\n",
    "        return\n",
    "\n",
    "    data_module = BoltzInferenceDataModule( # Using inference datamodule to get a batch\n",
    "        manifest=processed_input_data.manifest,\n",
    "        target_dir=processed_input_data.targets_dir,\n",
    "        msa_dir=processed_input_data.msa_dir,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    data_module.setup(stage='predict') \n",
    "\n",
    "    if checkpoint is None:\n",
    "        checkpoint_path = cache / \"boltz1_conf.ckpt\"\n",
    "    else:\n",
    "        checkpoint_path = Path(checkpoint)\n",
    "    \n",
    "    click.echo(f\"Loading model from checkpoint: {checkpoint_path}\")\n",
    "    # Predict_args for Boltz1 loading - may not be strictly necessary for fwd/bwd\n",
    "    # but kept for consistency with how model might expect to be loaded.\n",
    "    # These relate to *what* predict_step computes/returns.\n",
    "    predict_args_for_model_load = {\n",
    "        \"recycling_steps\": recycling_steps,\n",
    "        \"sampling_steps\": sampling_steps,\n",
    "        \"diffusion_samples\": diffusion_samples,\n",
    "        \"write_confidence_summary\": True, # To ensure predict_step returns enough data\n",
    "        \"write_full_pae\": True,\n",
    "        \"write_full_pde\": True,\n",
    "    }\n",
    "    diffusion_params = BoltzDiffusionParams()\n",
    "    diffusion_params.step_scale = step_scale\n",
    "\n",
    "    model_module: Boltz1 = Boltz1.load_from_checkpoint(\n",
    "        checkpoint_path,\n",
    "        strict=True, # Be strict about loading\n",
    "        predict_args=predict_args_for_model_load,\n",
    "        map_location=\"cpu\", # Load to CPU first\n",
    "        diffusion_process_args=asdict(diffusion_params),\n",
    "        ema=False, # Assuming no EMA for typical training profiling\n",
    "         steering_args={\"fk_steering\": False,\n",
    "                       \"guidance_update\": False,\n",
    "                       \"num_particles\": 3,\n",
    "                       \"fk_lambda\": 4.0},\n",
    "    )\n",
    "    model_module.to(torch_device)\n",
    "    model_module.train() # Set model to training mode\n",
    "    click.echo(\"Model loaded, moved to device, and set to train mode.\")\n",
    "\n",
    "    # Get a single batch\n",
    "    try:\n",
    "        dataloader = data_module.predict_dataloader()\n",
    "        if not dataloader: # Should not happen if setup was okay\n",
    "             click.secho(\"Predict dataloader is empty. Cannot get a batch.\", fg=\"red\")\n",
    "             return\n",
    "        batch = next(iter(dataloader))\n",
    "    except Exception as e:\n",
    "        click.secho(f\"Error getting a batch: {e}\", fg=\"red\")\n",
    "        return\n",
    "\n",
    "    # Move batch to the target device (assuming batch is a dict or list/tuple of tensors)\n",
    "    if isinstance(batch, dict):\n",
    "        batch = {k: v.to(torch_device) if hasattr(v, 'to') else v for k, v in batch.items()}\n",
    "    elif isinstance(batch, (list, tuple)):\n",
    "        batch = [b.to(torch_device) if hasattr(b, 'to') else b for b in batch]\n",
    "    elif hasattr(batch, 'to'): # Single tensor batch\n",
    "        batch = batch.to(torch_device)\n",
    "    click.echo(\"Single batch obtained and moved to device.\")\n",
    "\n",
    "    # Dummy optimizer (required for `loss.backward()` if parameters are involved)\n",
    "    # Filter for parameters that require gradients\n",
    "    trainable_params = filter(lambda p: p.requires_grad, model_module.parameters())\n",
    "    optimizer = torch.optim.Adam(trainable_params, lr=1e-4) # lr doesn't matter for this\n",
    "    optimizer.zero_grad() # Zero out gradients\n",
    "\n",
    "    click.echo(\"Starting CUDA memory recording for simulated training step...\")\n",
    "    torch.cuda.synchronize(torch_device) # Wait for all kernels to finish before starting\n",
    "    torch.cuda.reset_peak_memory_stats(torch_device) # Reset peak memory counter\n",
    "    \n",
    "    # Start recording memory history\n",
    "    # This is a global recorder, affecting the specified device or default CUDA device.\n",
    "    torch.cuda.memory._record_memory_history(enabled=True, device=torch_device)\n",
    "\n",
    "    # --- Forward Pass ---\n",
    "    # Use predict_step as it's a known interface for Boltz1 to get outputs.\n",
    "    # These outputs will be used to form a dummy loss.\n",
    "    # predict_step should be run with torch.enable_grad() if it's not already on.\n",
    "    # (We did torch.set_grad_enabled(True) globally earlier)\n",
    "    predictions_dict = model_module.predict_step(batch, batch_idx=0)\n",
    "    \n",
    "    # Create a dummy loss from one of the output tensors\n",
    "    loss = None\n",
    "    if isinstance(predictions_dict, dict):\n",
    "        for key, value in predictions_dict.items():\n",
    "            if isinstance(value, torch.Tensor) and value.is_floating_point():\n",
    "                # Ensure the tensor requires grad if it's an output of ops involving weights\n",
    "                # If value itself doesn't have grad_fn, its sum might not create a graph to backprop.\n",
    "                # A simple mean() should be fine if it's a result of model computations.\n",
    "                try:\n",
    "                    loss = value.abs().mean() # Take mean of absolute values to ensure it's scalar and positive\n",
    "                    click.echo(f\"Using output tensor from key '{key}' for dummy loss (shape: {value.shape}).\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    click.echo(f\"Could not use tensor from key {key} for loss: {e}\")\n",
    "        if loss is None:\n",
    "            click.secho(\"Error: Could not find a suitable tensor in predict_step output to form a dummy loss.\", fg=\"red\")\n",
    "            torch.cuda.memory._record_memory_history(enabled=False, device=torch_device)\n",
    "            return\n",
    "    elif isinstance(predictions_dict, torch.Tensor) and predictions_dict.is_floating_point():\n",
    "        loss = predictions_dict.abs().mean()\n",
    "        click.echo(f\"Using the direct tensor output from predict_step for dummy loss (shape: {predictions_dict.shape}).\")\n",
    "    else:\n",
    "        click.secho(\"Error: predict_step output is not a dict or tensor suitable for dummy loss.\", fg=\"red\")\n",
    "        torch.cuda.memory._record_memory_history(enabled=False, device=torch_device)\n",
    "        return\n",
    "\n",
    "    click.echo(f\"Dummy loss computed: {loss.item()}\")\n",
    "\n",
    "    # --- Backward Pass ---\n",
    "    loss.backward()\n",
    "    click.echo(\"Backward pass completed.\")\n",
    "\n",
    "    # (Optional: optimizer.step() would go here if you wanted to profile it too)\n",
    "    # optimizer.step() \n",
    "    # click.echo(\"Optimizer step completed.\")\n",
    "\n",
    "    torch.cuda.synchronize(torch_device) # Ensure all ops are done before dumping/stopping\n",
    "\n",
    "    # --- Dump Snapshot and Stop Recording ---\n",
    "    snapshot_file_name = \"training_memory_snapshot.pickle\"\n",
    "    snapshot_path = profile_out_path / snapshot_file_name\n",
    "    try:\n",
    "        torch.cuda.memory._dump_snapshot(str(snapshot_path))\n",
    "        click.echo(f\"CUDA memory snapshot saved to: {snapshot_path}\")\n",
    "    except Exception as e:\n",
    "        click.secho(f\"Error dumping snapshot: {e}\", fg=\"red\")\n",
    "    finally:\n",
    "        # Stop recording memory history\n",
    "        torch.cuda.memory._record_memory_history(enabled=False, device=torch_device)\n",
    "        click.echo(\"CUDA memory recording stopped.\")\n",
    "\n",
    "    # --- Report Peak Memory ---\n",
    "    peak_memory_bytes = torch.cuda.max_memory_allocated(torch_device)\n",
    "    click.echo(f\"Peak CUDA memory allocated during the step: {peak_memory_bytes / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # Clean up (optional, helps if running multiple times in a session)\n",
    "    del batch, loss, predictions_dict, model_module, data_module, optimizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    click.echo(\"--- Training Step CUDA Memory Profiling Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1826ad98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training Step CUDA Memory Profiling ---\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 42\n",
      "Memory snapshot will be saved to: /insomnia001/depts/edu/COMSE6998/yy3448/hpml_of3/cuda_memory_snapshots\n",
      "Checking input data.\n",
      "Processing inputs for 1 structure(s)...\n",
      "Processing input data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating MSA for inputs_boltz_profiling/example_protein.fasta with 1 protein entities.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sleeping for 10s. Reason: PENDING\n",
      "Sleeping for 5s. Reason: RUNNING\n",
      "Sleeping for 5s. Reason: RUNNING\n",
      "Sleeping for 10s. Reason: RUNNING\n",
      "Sleeping for 7s. Reason: RUNNING\n",
      "Sleeping for 6s. Reason: RUNNING\n",
      "Sleeping for 6s. Reason: RUNNING\n",
      "Sleeping for 6s. Reason: RUNNING\n",
      "Sleeping for 10s. Reason: RUNNING\n",
      "Sleeping for 5s. Reason: RUNNING\n",
      "Sleeping for 10s. Reason: RUNNING\n",
      "Sleeping for 10s. Reason: RUNNING\n",
      "COMPLETE: 100%|██████████| 150/150 [elapsed: 01:36 remaining: 00:00]\n",
      "100%|██████████| 1/1 [01:36<00:00, 96.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint: boltz_model_cache_training_profile/boltz1_conf.ckpt\n",
      "Model loaded, moved to device, and set to train mode.\n",
      "Single batch obtained and moved to device.\n",
      "Starting CUDA memory recording for simulated training step...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/insomnia001/depts/edu/COMSE6998/yy3448/mambaforge/envs/boltz/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using output tensor from key 'masks' for dummy loss (shape: torch.Size([1, 2528])).\n",
      "Dummy loss computed: 0.9988133311271667\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(const, \u001b[33m'\u001b[39m\u001b[33mmax_paired_seqs\u001b[39m\u001b[33m'\u001b[39m): const.max_paired_seqs = \u001b[32m128\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(const, \u001b[33m'\u001b[39m\u001b[33mmax_msa_seqs\u001b[39m\u001b[33m'\u001b[39m): const.max_msa_seqs = \u001b[32m256\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mprofile_training_step_cuda_memory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_path_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dir_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_base_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_cache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# checkpoint=None, # Uses default from cache\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# MUST BE GPU\u001b[39;49;00m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverride\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Re-process inputs each time for consistency in profiling\u001b[39;49;00m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed_val\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_msa_server\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# For dummy example, set to true if MMseqs2 not local\u001b[39;49;00m\n\u001b[32m     42\u001b[39m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# Ensure your FASTA header and parse_fasta setup allows this\u001b[39;49;00m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprofile_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprofiler_snapshot_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Boltz1 specific params, ensure they are sensible for your model\u001b[39;49;00m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecycling_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Reduced for faster test, may not be relevant for fwd/bwd of training\u001b[39;49;00m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiffusion_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 208\u001b[39m, in \u001b[36mprofile_training_step_cuda_memory\u001b[39m\u001b[34m(data_path_str, out_dir_str, cache_str, checkpoint, accelerator, num_workers, override, seed_val, use_msa_server, msa_server_url, msa_pairing_strategy, profile_output_dir, recycling_steps, sampling_steps, diffusion_samples, step_scale)\u001b[39m\n\u001b[32m    205\u001b[39m click.echo(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDummy loss computed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# --- Backward Pass ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m click.echo(\u001b[33m\"\u001b[39m\u001b[33mBackward pass completed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# (Optional: optimizer.step() would go here if you wanted to profile it too)\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# optimizer.step() \u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# click.echo(\"Optimizer step completed.\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/insomnia001/depts/edu/COMSE6998/yy3448/mambaforge/envs/boltz/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/insomnia001/depts/edu/COMSE6998/yy3448/mambaforge/envs/boltz/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/insomnia001/depts/edu/COMSE6998/yy3448/mambaforge/envs/boltz/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Cell for Main execution block (example)\n",
    "# Create dummy input file for the example\n",
    "example_input_dir = Path(\"./inputs_boltz_profiling\")\n",
    "example_input_dir.mkdir(parents=True, exist_ok=True)\n",
    "example_fasta_file = example_input_dir / \"example_protein.fasta\"\n",
    "\n",
    "if not example_fasta_file.exists():\n",
    "    with open(example_fasta_file, \"w\") as f:\n",
    "        # Using a header format that should trigger MSA generation if parse_fasta sets msa_id=None -> ChainInfo.msa_id=0\n",
    "        f.write(\">CHAIN_X|protein\\n\") # MSA_ID field omitted to trigger generation via default\n",
    "        f.write(\"MGKVKVGVNGFGRIGRLVTRAAFNSGKVDIVAINDPFIDLNYMVYMFQYDSTHGKFHGTVK\\n\")\n",
    "        f.write(\"AENGKLVINGMPTGIILLTEPVEDRAMAKAKAEMTGKEIKAAQNIIPSSTGAAKAVGKVLP\\n\")\n",
    "        f.write(\"ELGKLTGMAFRVPTANVSVVDLTCRLEKPAKYDDIKKVVKQASEGPLKGILGYTEHQVVSS\\n\")\n",
    "        f.write(\"DFNSDTHSSTFDAGAGIALNDHFVKLISWYDNEFGYSNRVVDLMAHMASKEALGGENGLYL\\n\")\n",
    "        f.write(\"IHGSNVTANYLPADDRVRYTLYTIAALLGLSLFKGAKVGILNVSADCGLTDAFHQLDSLLG\\n\")\n",
    "        f.write(\"GRRALKNIVIPTSTGAAKAHEIVLKAGQHAA\\n\") # Example: GAPDH_HUMAN sequence (335 AA)\n",
    "    print(f\"Created dummy input file: {example_fasta_file}\")\n",
    "\n",
    "# --- Parameters for the training step profiling ---\n",
    "input_data = str(example_fasta_file)\n",
    "output_base_dir = \"./outputs_boltz_training_profile\" # General output for processed data etc.\n",
    "profiler_snapshot_dir = \"./cuda_memory_snapshots\" # Specific for memory .pickle file\n",
    "model_cache_dir = \"./boltz_model_cache_training_profile/\"\n",
    "\n",
    "# Ensure const module has placeholder values if not fully set up\n",
    "# This is for the `process_inputs` part if `boltz.data.const` is not fully available\n",
    "if not hasattr(const, 'chain_type_ids'): const.chain_type_ids = {\"PROTEIN\": 0, \"RNA\": 1} # Simplified\n",
    "if not hasattr(const, 'max_paired_seqs'): const.max_paired_seqs = 128\n",
    "if not hasattr(const, 'max_msa_seqs'): const.max_msa_seqs = 256\n",
    "\n",
    "\n",
    "profile_training_step_cuda_memory(\n",
    "    data_path_str=input_data,\n",
    "    out_dir_str=output_base_dir,\n",
    "    cache_str=model_cache_dir,\n",
    "    # checkpoint=None, # Uses default from cache\n",
    "    accelerator=\"gpu\", # MUST BE GPU\n",
    "    num_workers=0,\n",
    "    override=True, # Re-process inputs each time for consistency in profiling\n",
    "    seed_val=42,\n",
    "    use_msa_server=True, # For dummy example, set to true if MMseqs2 not local\n",
    "                            # Ensure your FASTA header and parse_fasta setup allows this\n",
    "    profile_output_dir=profiler_snapshot_dir,\n",
    "    # Boltz1 specific params, ensure they are sensible for your model\n",
    "    recycling_steps=1, \n",
    "    sampling_steps=10, # Reduced for faster test, may not be relevant for fwd/bwd of training\n",
    "    diffusion_samples=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da544a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
